# OSSBIG AI engineering team 2024

This is the central repository for the OSSBIG AI engineering team.

# Workbooks and Repositories

- CUDA acceleration in C: https://colab.research.google.com/drive/1N50mFGvlg0CFGDONdsS0ZQL2NyJEAMMt#scrollTo=HLdoj4cz-xal
- CUDA code repository (Branch CUDA): https://github.com/mankra/llama2.c
- Modified IntelliJ Plugin (Braanch KRM): https://github.com/mankra/llm-intellij

# Useful Links

- Benchmark Referenz: https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
- Neue Ver√∂ffentlichung: https://opencodeinterpreter.github.io/#example
- GPT from Numpy: https://jaykmody.com/blog/gpt-from-scratch/

# Karpathy - Build GPT From Scratch

- Lets build the GPT Tokenizer: https://youtu.be/zduSFxRajkE?si=oiY3IYLJjNB3nR86
- The spelled-out intro to language modeling: building makemore: https://youtu.be/PaCmpygFfXo?si=Xo-7vP22l-I6l028
- Building makemore Part 2: MLP: https://youtu.be/TCH_1BHY58I?si=Ozu9nNz6EAiBMAFP
- Building makemore Part 3: Activations & Gradients, BatchNorm: https://youtu.be/P6sfmUTpUmc?si=qNfH2TI5QzDh4gVa
- Building makemore Part 4: Becoming a Backprop Ninja: https://youtu.be/q8SA3rM6ckI?si=_50lnWJpjG4iwknc


## Finetuning workbooks and repositories - not successfull

- Finetuning Code Repository (Data preperation, training): https://github.com/mankra/finetune
- Finetuning Workbook for endless-sky game: https://colab.research.google.com/drive/18XqYBVkJv9lqwrnWuoI06ZUxQUuwiR1U#scrollTo=4AXJGt-ag4l5
- Finetuning Workbook for fltk: https://colab.research.google.com/drive/1dXruWXemEYI7srTbNAjEe_ESEvRFGBBT#scrollTo=v8MG0g-eEW9X
- Merge PEFT Layer with base model: https://colab.research.google.com/drive/1GWCLdRmZNXh6CbOMAwou4FRQCe19K-8x#scrollTo=dOIeFZfbiQmz
